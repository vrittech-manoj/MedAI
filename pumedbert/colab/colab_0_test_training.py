# -*- coding: utf-8 -*-
"""pubmedbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zArQYQf9uzh1D8VnBUSH2UnST8jUxID7
"""

# Install transformers and related libraries
!pip install transformers datasets torch torchvision torchaudio
!pip install accelerate
!pip install scikit-learn pandas numpy matplotlib seaborn
!pip install wandb  # Optional: for experiment tracking

!pip install -U transformers datasets
!pip install -U transformers datasets

import torch
import pandas as pd
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Load PubMedBERT tokenizer and model
model_name = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# For feature extraction (embeddings)
base_model = AutoModel.from_pretrained(model_name)

print(f"Model loaded: {model_name}")
print(f"Vocabulary size: {tokenizer.vocab_size}")
print(f"Max sequence length: {tokenizer.model_max_length}")

# Test tokenization with biomedical text
sample_text = "The patient presented with acute myocardial infarction and was treated with percutaneous coronary intervention."

# Tokenize
tokens = tokenizer.tokenize(sample_text)
token_ids = tokenizer.encode(sample_text, add_special_tokens=True)

print("Original text:", sample_text)
print("Tokens:", tokens)
print("Token IDs:", token_ids)
print("Decoded:", tokenizer.decode(token_ids))

#Step 3: Prepare Sample Dataset
#3.1 Create Sample Biomedical Text Classification Dataset
# Sample biomedical text classification data
# In practice, you would load your own dataset
# sample_data = {
#     'text': [
#         "The patient was diagnosed with type 2 diabetes mellitus and prescribed metformin.",
#         "Acute respiratory distress syndrome was observed in the ICU patient.",
#         "The study shows promising results for cancer immunotherapy treatments.",
#         "Hypertension management requires lifestyle modifications and medication adherence.",
#         "COVID-19 symptoms include fever, cough, and difficulty breathing.",
#         "Alzheimer's disease progression can be slowed with early intervention.",
#         "The antibiotic treatment was effective against the bacterial infection.",
#         "Cardiovascular exercise reduces the risk of heart disease significantly.",
#         "Gene therapy shows potential for treating inherited genetic disorders.",
#         "Mental health awareness is crucial for overall patient wellbeing."
#     ],
#     'label': [0, 1, 2, 0, 1, 3, 1, 0, 2, 3]  # 0: General, 1: Acute Care, 2: Research, 3: Chronic Care
# }
sample_data = {
    'text': [
        # General (0)
        "The patient was diagnosed with type 2 diabetes mellitus and prescribed metformin.",
        "Hypertension management requires lifestyle modifications and medication adherence.",
        "Cardiovascular exercise reduces the risk of heart disease significantly.",
        "Proper sleep hygiene contributes to better overall health and mental clarity.",
        "Regular health checkups help in early disease detection and prevention.",
        "Nutritional counseling is essential for promoting lifelong healthy habits.",
        "Vaccination programs have improved public health outcomes globally.",
        "Primary care physicians play a key role in ongoing health maintenance.",

        # Acute Care (1)
        "Acute respiratory distress syndrome was observed in the ICU patient.",
        "COVID-19 symptoms include fever, cough, and difficulty breathing.",
        "The antibiotic treatment was effective against the bacterial infection.",
        "Emergency intubation was required due to severe respiratory failure.",
        "The trauma patient was stabilized and transferred to the surgical ICU.",
        "Rapid administration of epinephrine reversed the anaphylactic reaction.",
        "The ER team responded immediately to the cardiac arrest call.",
        "Sepsis protocol was initiated for the patient with high fever and hypotension.",

        # Research (2)
        "The study shows promising results for cancer immunotherapy treatments.",
        "Gene therapy shows potential for treating inherited genetic disorders.",
        "Clinical trials are ongoing for a new Alzheimerâ€™s medication.",
        "Researchers identified novel biomarkers for early cancer detection.",
        "A new vaccine platform showed strong immune response in lab animals.",
        "Machine learning models were used to predict disease progression.",
        "The trial tested the efficacy of stem cell therapy in spinal injury patients.",

        # Chronic Care (3)
        "Alzheimer's disease progression can be slowed with early intervention.",
        "Mental health awareness is crucial for overall patient wellbeing.",
        "Long-term asthma control requires consistent medication use and monitoring.",
        "Regular dialysis sessions are essential for end-stage renal disease patients.",
        "Patients with rheumatoid arthritis benefit from biologic therapy.",
        "Chronic pain management often includes both physical and psychological support.",
        "Diabetes management includes glucose monitoring and dietary planning.",
        "Adherence to HIV therapy significantly improves long-term outcomes.",
    ],
    'label': [
        0, 0, 0, 0, 0, 0, 0, 0,  # General
        1, 1, 1, 1, 1, 1, 1, 1,  # Acute Care
        2, 2, 2, 2, 2, 2, 2,     # Research
        3, 3, 3, 3, 3, 3, 3, 3   # Chronic Care
    ]
}

# Create DataFrame
df = pd.DataFrame(sample_data)

# Label mapping
label_names = {0: "General", 1: "Acute Care", 2: "Research", 3: "Chronic Care"}
df['label_name'] = df['label'].map(label_names)

print("Dataset shape:", df.shape)
print("\nLabel distribution:")
print(df['label_name'].value_counts())
print("\nSample data:")
print(df.head())

# Split into train and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['label'].tolist(),
    test_size=0.4,
    random_state=42,
    stratify=df['label'].tolist()
)
# train_texts = df['text'].tolist()
# train_labels = df['label'].tolist()

print("train_texts", train_texts)
print("train_labels", val_texts)
print("train_labels", train_labels)
print("train_labels", val_labels)


print(f"Training samples: {len(train_texts)}")
print(f"Validation samples: {len(val_texts)}")
print(f"Number of validation samples: {int(len(df) * 0.4)}")

#Step 4: Data Preprocessing and Tokenization
#4.1 Create Tokenization Function
def tokenize_function(examples):
    """Tokenize the input texts"""
    return tokenizer(
        examples['text'],
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    )

# Create datasets
train_dataset = Dataset.from_dict({
    'text': train_texts,
    'labels': train_labels
})

# val_dataset = Dataset.from_dict({
#     'text': val_texts,
#     'labels': val_labels
# })

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

print("Tokenization completed!")
print("Train dataset features:", train_dataset.features)

# Number of labels in your classification task
num_labels = len(set(train_labels))

# Load model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    id2label={i: label_names[i] for i in range(num_labels)},
    label2id={label_names[i]: i for i in range(num_labels)}
)

# Move model to device
model.to(device)

print(f"Model loaded for classification with {num_labels} labels")
print("Label mapping:", model.config.id2label)

# Data collator for dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def compute_metrics(eval_pred):
    """Compute metrics for evaluation"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    accuracy = accuracy_score(labels, predictions)

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Training arguments
training_args = TrainingArguments(
    output_dir='./pubmedbert-classifier',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    # Changed evaluation_strategy value to a string
    eval_strategy="epoch", # Changed from evaluation_strategy
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    save_total_limit=2,
    seed=42,
    fp16=False,  # Mixed precision training
)

print("Training arguments configured!")

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("Trainer initialized successfully!")

# Start training
print("Starting training...")
trainer.train()

print("Training completed!")

# Evaluate on validation set
print("Evaluating model...")
eval_results = trainer.evaluate()

print("Evaluation Results:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

def predict_text(text, model, tokenizer):
    """Make prediction on a single text"""
    # Tokenize input
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    # Move to device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Make prediction
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()

    return predicted_class, confidence

# Test predictions
test_texts = [
    "The patient underwent emergency cardiac catheterization for acute STEMI.",
    "Recent clinical trials demonstrate efficacy of novel immunotherapy approaches.",
    "Chronic kidney disease management requires multidisciplinary care coordination.",
    "The patient was diagnosed with type 2 diabetes mellitus and prescribed metformin.",
]

print("Test Predictions:")
print("-" * 50)
for text in test_texts:
    pred_class, confidence = predict_text(text, model, tokenizer)
    pred_label = label_names[pred_class]
    print(f"Text: {text}")
    print(f"Predicted: {pred_label} (confidence: {confidence:.3f})")
    print()

